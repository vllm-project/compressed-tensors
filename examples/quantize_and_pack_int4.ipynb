{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W4A16 Quantization and Compression ##\n",
    "\n",
    "Using compressed-tensors, we can compress a quantized model to store it more efficiently on disk.\n",
    "\n",
    "In this example, we run post-training quantization (PTQ) to quantize the weights of an example model to 4 bits. We then save a compressed version of the model on disk by packing each group of eight 4-bit weights into a single int32\n",
    "\n",
    "By packing groups of eight 4-bit weights into a single int32, we can store a quantized model more efficiently on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from compressed_tensors.quantization import (\n",
    "    QuantizationConfig,\n",
    "    QuantizationStatus,\n",
    "    apply_quantization_config\n",
    ")\n",
    "from compressed_tensors.compressors import ModelCompressor\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DefaultDataCollator\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a dense, unquantized tiny llama model\n",
    "device = \"cuda:0\"\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device, torch_dtype=torch.bfloat16)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following quantization config will be used to quantize all of the Linear layers to 4 bits, excluding the lm_head layer. \n",
    "\n",
    "The `format` argument is set to `pack-quantized`, indicating that when the model is saved we should use the `PackedQuantizationCompressor` which will pack every eight 4-bit weights into an `int32`. \n",
    "\n",
    "This will give us a compression ratio of 8x on each Linear layer compared to the unquantized `float32` representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config_dict = {\n",
    "\t\"quant_method\": \"compressed-tensors\",\n",
    "\t\"format\": \"pack-quantized\",\n",
    "\t\"global_compression_ratio\": None,\n",
    "\t\"config_groups\": {\n",
    "        \"group_1\": {\n",
    "            \"weights\": {\n",
    "                \"num_bits\": 4,\n",
    "                \"type\": \"int\",\n",
    "                \"symmetric\": False,\n",
    "                \"strategy\": \"tensor\"\n",
    "            },\n",
    "            \"targets\": [\"Linear\"]\n",
    "        }\n",
    "    },\n",
    "\t\"ignore\": [\"lm_head\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the loaded model for quantization calibration\n",
    "\n",
    "config = QuantizationConfig(**quantization_config_dict)\n",
    "config.quantization_status = QuantizationStatus.CALIBRATION\n",
    "apply_quantization_config(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataloader of calibration data\n",
    "\n",
    "dataset = load_dataset(\"ptb_text_only\", trust_remote_code=True)[\"train\"]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=False, truncation=True, max_length=1024)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    tokenized_dataset, batch_size=1, collate_fn=DefaultDataCollator(), sampler=RandomSampler(tokenized_dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running calibration: 512it [00:58,  8.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# calibrate scale and zero points for quantization using a small amount of train data\n",
    "num_calibration_samples = 512\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, sample in tqdm(enumerate(data_loader), desc=\"Running calibration\"):\n",
    "        sample = {key: value.to(device) for key,value in sample.items()}\n",
    "        _ = model(**sample)\n",
    "\n",
    "        if idx >= num_calibration_samples:\n",
    "            break\n",
    "\n",
    "# freeze scale and zero points after calibration\n",
    "# model.apply(freeze_module_quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running calibration, each quantized layer will have a new scale and zero_point parameter as shown below.\n",
    "\n",
    "Notice that at this point, the weight itself is still a floating point and has not been quantized. \n",
    "\n",
    "To convert the weights to an integer type, we need to apply the `compress_model` function. After compressing the weights, a forward pass of the model can no longer be run in PyTorch.\n",
    "\n",
    "After compressing the quantized model with the `pack-quantized` format, weights are represented as logical int4 values packed into int32 containers ( `weight_packed` ), with the original shape recorded in `weight_shape`.\n",
    "\n",
    "This packed representation is what gets saved to disk when using ModelCompressor.compress_model(model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale: tensor([-3.0465e+26], device='cuda:0', dtype=torch.bfloat16), Zero Point: tensor([0], device='cuda:0', dtype=torch.int8)\n",
      "Weight min: -1.5859375 max: 1.03125 dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "example_layer = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "scale = state_dict[example_layer + \"_scale\"]\n",
    "zero_point = state_dict[example_layer + \"_zero_point\"]\n",
    "weight = state_dict[example_layer]\n",
    "print(f\"Scale: {scale}, Zero Point: {zero_point}\")\n",
    "print(f\"Weight min: {torch.min(weight)} max: {torch.max(weight)} dtype: {weight.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing model: 154it [00:02, 59.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed weight scale: tensor([-3.0465e+26], device='cuda:0', dtype=torch.bfloat16), zero point: tensor([0], device='cuda:0', dtype=torch.int8)\n",
      "Compressed weight  dtype: torch.int32\n",
      "Compressed weight shape: torch.Size([2048, 256])\n",
      "Uncompressed weight shape: tensor([2048, 2048], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# convert quantized weights to integers\n",
    "compressor = ModelCompressor(quantization_config=config)\n",
    "compressor.compress_model(model)\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "example_layer = \"model.layers.0.self_attn.q_proj.weight\"\n",
    "scale = state_dict[example_layer + \"_scale\"]\n",
    "zero_point = state_dict[example_layer + \"_zero_point\"]\n",
    "weight = state_dict[example_layer + \"_packed\"]\n",
    "shape = state_dict[example_layer + \"_shape\"]\n",
    "print(f\"Compressed weight scale: {scale}, zero point: {zero_point}\")\n",
    "print(f\"Compressed weight  dtype: {weight.dtype}\")\n",
    "print(f\"Compressed weight shape: {weight.shape}\")\n",
    "print(f\"Uncompressed weight shape: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression format: pack-quantized\n",
      "Size of the model's weights on disk using safetensors: 712.25 MB\n"
     ]
    }
   ],
   "source": [
    "# apply compression and save the model to disk\n",
    "\n",
    "output_dir = \"./ex_llama1.1b_w4a16_packed_quantize\"\n",
    "compression_format = config.format\n",
    "print(f\"Compression format: {compression_format}\")\n",
    "\n",
    "\n",
    "model.save_pretrained(output_dir, state_dict=model.state_dict())\n",
    "compressor.update_config(output_dir)\n",
    "\n",
    "compressed_size_on_disk_mb = os.path.getsize(os.path.join(output_dir, \"model.safetensors\")) / 1024 / 1024\n",
    "print(f\"Size of the model's weights on disk using safetensors: {compressed_size_on_disk_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
